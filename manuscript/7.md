# Problem 7: On Parsing Strings
This one's less of a "broken script" and more of a problem that tests your knowledge of PowerShell toolmaking patterns. Suppose you have a data file that consists of multiple lines. Each line looks like this:

```
NM1*PR*2*MEDICARE COMPLETE*****PI*map1stString
```

There's no header, but each `*` denotes a field. You'll notice that some of the fields are empty; that isn't the case on every single line, but it's something you need to consider. Your immediate need is to read these in, and where the fourth field reads "MEDICARE COMPLETE," you need to change it to "MEDICARE ALL," and then re-write all the lines back out to the data file. 

How would you do this?

## Spoiler!!
A lot of folks tackle this as a string-parsing problem, and they're not wrong to do so. However, what I see a lot of people do is spend a lot of time parsing out the data they want. For example:

```
ForEach ($line in (Get-Content file.txt)) {
 If ($line -like '*MEDICARE COMPLETE*') {
  Write $line -replace 'MEDICARE COMPLETE','MEDICARE ALL'
 } else {
  Write $line
 }
}
```
 
Or something along those lines. This _kind of_ misses the point of PowerShell, which is to _act as a wrapper around other, hard-to-do stuff_. PowerShell takes stuff which isn't structured, like text, and turns it into structured objects. I would start by writing myself a set of `Import-` and `Export-` commands which respectively read-in that text and created objects, and wrote-out those objects into the desired string state. In _this_ case, the hard work's already been done:

```
Function Import-MyDataFile {
 [CmdletBinding()]
 Param(
  [Parameter(Mandatory=$True)]
  [string]$FilePath
 )
 $headers = @('Code','Type','Value',
  'Name','Amount','Extended','Allowed','Denied',
  'System','Note')
 Import-CSV $FilePath -Delim "*" -Header $headers
}

Function Export-MyDataFile {
 [CmdletBinding()]
 Param(
  [Parameter(Mandatory=$True)]
  [string]$FilePath,
  [Parameter(ValueFromPipeline=$True)]
  [string[]]$InputObject
 )
 BEGIN {
  $data = @()
 }
 PROCESS {
  $data += $InputObject
 }
 END {
  $InputObject | Export-CSV $FilePath Delim '*' -NoHead
 }
}
```

See, the `-CSV` commands can already handle any kind of delimited data, so I've just "wrapped" them into a version specific to my data, which uses `*` as delimiters.  I've also specified headers (which I made up for this example, but presumably in a production environement someone could tell me what each field meant), so I can have a convenient way of referring to my data. Having done that:

```
Import-MyDataFile input.txt |
ForEach {
 $_.Name = $_.Name -Replace `
 'MEDICARE COMPLETE','MEDICARE ALL'
} |
Export-MyDataFile output.txt
```

Or something broadly _like_ all that. The syntax here isn't my point so much as the _pattern_. If you're going to engage in text-parsing, then always do so with the goal of producing _objects_, rather than directly manipulating text. Objects become easier to work with since PowerShell is geared around objects (and is very much not geared around text). Having done this work to parse strings into objects and back again, future tasks working with this same data structure will be vastly faster and simpler, meaning I get a big return on my investment.

All this applies, in my mind, _anytime_ I'm looking at text files. For example, I see people _killing themselves_ parsing log files, trying to find a line that contains some specific text, which is often partly variable, so they end up writing tortuous regular expressions. I'd rather front-load all that work, parse the entire file into _objects_, and then deal with the objects. PowerShell _loves_ structured data (objects) and is much better at working with them than it is dealing with huge wedges of text.

Bear in mind that `ConvertFrom-String` exists in newer versions of Windows PowerShell, and is very powerful at turning unstructured text into structured data with a lot less work on your part. Use what's in the shell to reduce your workload!

